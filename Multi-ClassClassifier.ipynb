{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2 as cv\n",
    "from PIL import Image , ImageOps\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "from resizeimage import resizeimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "   x (103, 1600) \n",
      "   y (103, 14)\n",
      "test set:\n",
      "   x (51, 1600) \n",
      "   y (51, 14)\n"
     ]
    }
   ],
   "source": [
    "####  DATA PREP ##################################\n",
    "\n",
    "directory = os.listdir('data/') #saves all file names to array\n",
    "random.shuffle(directory) #randomize / shuffle elements\n",
    "\n",
    "f = []\n",
    "y = []\n",
    "df = np.zeros((154,1600))\n",
    "\n",
    "\n",
    "for im in directory:\n",
    "    \n",
    "    if(im[0] != '.'):\n",
    "        path = 'data/'+im \n",
    "        with Image.open(path) as image:\n",
    "            resized = resizeimage.resize_cover(image, [40, 40])\n",
    "            resized.save('resized', image.format) \n",
    "        y.append(int(re.findall(r'\\d+', im)[0])-2) #extract subject num to assign as target class\n",
    "        f.append(np.array(resized))\n",
    "\n",
    "d = np.array(f)\n",
    "d = np.divide(d, 255) #division by 255 to keep vals in range (0,1\n",
    "\n",
    "#flatten observations into rows of 1x1600\n",
    "for i in range(154):\n",
    "    df[i] = np.ndarray.flatten(d[i])\n",
    "\n",
    "#truncate vals to remain in range (0,1)\n",
    "df[df<0] = 0\n",
    "df[df>1] = 1\n",
    "\n",
    "#one hot encoding matrix creation - target class labels\n",
    "one_hot = np.zeros((np.shape(d)[0],len(np.unique(y))))\n",
    "i = 0\n",
    "\n",
    "for c in y:\n",
    "    one_hot[i][c] = 1\n",
    "    i+=1\n",
    "\n",
    "#split into train set (103x1600) & validation set (51x1600)\n",
    "x_train = df[:103:]\n",
    "y_train = one_hot[:103:]\n",
    "x_test = df[-51:]\n",
    "y_test = one_hot[-51:]\n",
    "\n",
    "print(\"train set:\\n   x\", np.shape(x_train), \"\\n   y\", np.shape(y_train ))\n",
    "print(\"test set:\\n   x\", np.shape(x_test), \"\\n   y\", np.shape(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  MULTI-CLASS CLASSIFIER ###########################\n",
    "# INPUT -> FC -> ACT (SIGMOID) -> OUT -> CROSS ENTROPY ACTIVATION\n",
    "\n",
    "def fc_layer(x, w, b):\n",
    "    '''Input -> 103 obs x 1600 features\n",
    "       w     -> 14 classes x 1600 features\n",
    "       b     -> 14 classes\n",
    "       \n",
    "       outputs 103 obs x 14 classes'''\n",
    "    \n",
    "    tmp = []\n",
    "\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        tmp.append(np.sum(x[i]*w+b, axis=1))\n",
    "    \n",
    "    return np.array(tmp)\n",
    "    \n",
    "\n",
    "def softmax(z):\n",
    "    \n",
    "    '''output valid probability dist'''\n",
    "    \n",
    "    tmp = []\n",
    "\n",
    "    #for ea observation\n",
    "    for i in range(np.shape(z)[0]):\n",
    "        tmp.append(np.exp(z[i] - np.max(z[i])) / ( np.sum(np.exp(z[i] - np.max(z[i]))) ))\n",
    "    \n",
    "    return np.array(tmp)\n",
    "    \n",
    "def compute_cost(y, yhat):\n",
    "    '''Cross Entropy\n",
    "       Input: y - target - one hot encoded - (103x14) \n",
    "              yhat - valid dist - (103x14)'''\n",
    "    tmp = []\n",
    "\n",
    "    #for ea observation\n",
    "    for i in range(len(y)):\n",
    "        tmp.append( -y[i] * np.log(yhat[i]) )\n",
    "    \n",
    "    return np.array(tmp)\n",
    "\n",
    "def compute_gradients(x, y, yhat):\n",
    "    \n",
    "    '''Gradient of softmax w cross entropy'''\n",
    "    \n",
    "    n=len(y)\n",
    "    \n",
    "    g_w = x_train.T@(yhat - y)\n",
    "    g_b = np.sum((yhat - y), axis=0)\n",
    "    \n",
    "    return g_w, g_b.reshape((14,1))\n",
    "\n",
    "def update_weights(w, b, g_w, g_b, lr):\n",
    "    \n",
    "    '''Increase gradient w restult from compute_gradient() to weights'''\n",
    "    \n",
    "    tmp_w = []\n",
    "    tmp_b = []\n",
    "\n",
    "    #for ea class\n",
    "    for i in range(14):\n",
    "        tmp_w.append( np.add(w[i],-(lr*g_w[:,i])) )\n",
    "        tmp_b.append( np.add(b[i],-(lr*g_b[i])) )\n",
    "\n",
    "    return np.array(tmp_w), np.array(tmp_b)   \n",
    "\n",
    "def terminate(j_p, j_c):\n",
    "    \n",
    "    return np.round(np.average(j_p), 4) == np.round(np.average(j_c), 4)\n",
    "\n",
    "def classify(y_hat, y):\n",
    "\n",
    "    '''Cutoff is 0.5 - any values that fall above are set to 1 and any below to 0'''\n",
    "    \n",
    "    classified = []\n",
    "    \n",
    "    for i in range(len(yhat)):\n",
    "\n",
    "        classified.append(np.argmax(yhat))\n",
    "    \n",
    "    return classified\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \n",
    "    tp = 0\n",
    "    err = 0\n",
    "\n",
    "    #for ea observation\n",
    "    for i in range(len(y)):\n",
    "\n",
    "        #if the highest prob class in yhat matches the target class\n",
    "        if( np.argmax(y[i]) == np.argmax(y_hat[i]) ):\n",
    "            tp += 1\n",
    "        else:\n",
    "            err += 1\n",
    "    \n",
    "    return tp, err\n",
    "\n",
    "def visualize(epochs, j):\n",
    "    \n",
    "    '''Charts J vs Epoch'''\n",
    "\n",
    "    epoch=np.linspace(1,epochs,num=epochs)\n",
    "\n",
    "    print(\"epoch vs J ->\")\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.plot(epoch, j)\n",
    "    axes.set_xlabel('epoch')\n",
    "    axes.set_ylabel('J')\n",
    "    plt.show()\n",
    "\n",
    "#####################################################\n",
    "\n",
    "#init weights\n",
    "w = np.random.uniform(-10**-4,10**-4, (14,1600))\n",
    "b = np.random.uniform(-10**-4,10**-4, (14,1))\n",
    "\n",
    "j_track = [] #tracks chng in cost function - sanity check\n",
    "l_rate = 10**-4\n",
    "j=0\n",
    "epochs = 0\n",
    "\n",
    "#GRADIENT DESCENT\n",
    "while(epochs < 1500):\n",
    "    \n",
    "    #FC LAYER - composed of fc + activation\n",
    "    z = fc_layer(x_train, w, b)\n",
    "    yhat = softmax(z)\n",
    "\n",
    "    #OUTPUT LAYER - evaluate cost function\n",
    "    j_prev = j #to track/plot loss function\n",
    "    j = compute_cost(y_train, yhat) \n",
    "    j_track.append(np.average(j))\n",
    "\n",
    "    #Update Weights\n",
    "    g_w, g_b = compute_gradients(x_train, y_train, yhat)\n",
    "    w, b = update_weights(w, b, g_w, g_b, l_rate)\n",
    "    \n",
    "    epochs += 1 \n",
    "#    if(terminate(j_prev,j)): break\n",
    "\n",
    "########################################################################\n",
    "\n",
    "#classify / compute accuracy\n",
    "ztest = fc_layer(x_test, w, b)\n",
    "yhat_test = softmax(ztest)\n",
    "tp, err = accuracy(yhat_test, y_test)\n",
    "print(\"Accuracy: \", (tp / (tp+err)))\n",
    "\n",
    "\n",
    "#Visualize epoch V J\n",
    "epoch=np.linspace(1,epochs,num=epochs)\n",
    "\n",
    "print(\"epoch vs J ->\")\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(epoch, j_track)\n",
    "axes.set_xlabel('epoch')\n",
    "axes.set_ylabel('J')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
